input {
  # Input for backend logs via TCP
  tcp {
    port => 5000
    codec => json_lines
    type => "backend"
    tags => ["vallmere", "backend", "nestjs"]
  }

  # Input for frontend logs via TCP  
  tcp {
    port => 5001
    codec => json_lines
    type => "frontend"
    tags => ["vallmere", "frontend", "angular"]
  }

  # Input for nginx logs via Filebeat
  beats {
    port => 5044
    type => "nginx"
    tags => ["vallmere", "nginx", "webserver"]
  }
}

filter {
  # Common fields for all logs
  mutate {
    add_field => { "environment" => "production" }
    add_field => { "service" => "vallmere" }
    add_field => { "version" => "1.0.0" }
  }

  # Backend logs processing
  if [type] == "backend" {
    # Parse timestamp
    if [timestamp] {
      date {
        match => [ "timestamp", "ISO8601" ]
        target => "@timestamp"
      }
    }

    # Add backend specific fields
    mutate {
      add_field => { "component" => "api" }
      add_field => { "technology" => "nestjs" }
    }

    # Parse log level
    if [level] {
      mutate {
        uppercase => [ "level" ]
      }
    }

    # Extract request information if available
    if [message] =~ /HTTP/ {
      grok {
        match => { "message" => "%{WORD:method} %{URIPATH:endpoint}" }
        tag_on_failure => ["grok_failure_backend_http"]
      }
    }
  }

  # Frontend logs processing
  if [type] == "frontend" {
    mutate {
      add_field => { "component" => "client" }
      add_field => { "technology" => "angular" }
    }

    # Parse user agent if available
    if [userAgent] {
      useragent {
        source => "userAgent"
        target => "browser"
      }
    }

    # Extract client IP if available
    if [clientIP] {
      mutate {
        add_field => { "client_ip" => "%{clientIP}" }
      }
    }
  }

  # Nginx logs processing
  if [type] == "nginx" {
    mutate {
      add_field => { "component" => "reverse-proxy" }
      add_field => { "technology" => "nginx" }
    }

    # Parse nginx access logs
    if [fields][log_type] == "access" {
      grok {
        match => { 
          "message" => "%{COMBINEDAPACHELOG}" 
        }
        tag_on_failure => ["grok_failure_nginx_access"]
      }

      # Convert response code to number
      if [response] {
        mutate {
          convert => { "response" => "integer" }
        }
      }

      # Convert bytes to number
      if [bytes] {
        mutate {
          convert => { "bytes" => "integer" }
        }
      }

      # Parse response time if available
      if [request_time] {
        mutate {
          convert => { "request_time" => "float" }
        }
      }
    }

    # Parse nginx error logs
    if [fields][log_type] == "error" {
      grok {
        match => { 
          "message" => "%{GREEDYDATA:error_message}" 
        }
        tag_on_failure => ["grok_failure_nginx_error"]
      }
    }
  }

  # Cleanup and enrichment
  mutate {
    remove_field => [ "host", "agent", "ecs", "input", "log" ]
  }

  # Add GeoIP information for client IPs
  if [clientip] {
    geoip {
      source => "clientip"
      target => "geoip"
    }
  }

  # Add timing information
  ruby {
    code => "
      event.set('processing_time', Time.now.to_f - event.get('@timestamp').to_f)
    "
  }
}

output {
  # Send to Elasticsearch with dynamic index names
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    # Dynamic index based on type and date
    index => "vallmere-%{type}-%{+YYYY.MM.dd}"
    # Retry settings
    retry_on_conflict => 3
    # Performance settings
    flush_size => 500
    idle_flush_time => 10
  }
  # Debug output (remove in production)
  stdout { 
    codec => rubydebug 
  }
} 